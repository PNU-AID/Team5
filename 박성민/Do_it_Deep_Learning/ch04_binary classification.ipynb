{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-1 초기 인공지능 알고리즘과 로지스틱 회귀"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 퍼셉트론\n",
    "* $$z=b+\\sum_{i=1}^{n}w_ix_i$$\n",
    "* 활성화 함수로는 계산 함수를 사용\n",
    "* 계단 함수의 결과를 역방향 계산하는데 쓴다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 아달린\n",
    "* 퍼셉트론을 개선한 적응형 선형 뉴런(Adaptive Linear Neuron)\n",
    "* 퍼셉트론과 다르게 선형 함수 출력 이후에 역방향 계산이 일어남"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 로지스틱 회귀\n",
    "* logistic regression 아달린에서 조금 더 발전한 형태\n",
    "* 활성화 함수 사용\n",
    "* 퍼셉트론의 계단 함수와 비슷한 임계 함수 사용, 임계 함수는 입력으로 활서오하 함수의 출력을 받음\n",
    "* 활성화 함수는 비선형 함수 사용"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-2 시그모이드 함수"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시그모이드 함수의 역할\n",
    "* 로지스틱 회귀의 활성화 함수로 사용\n",
    "* 시그모이드 함수를 통과하면 확률이 나온다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시그모이드 함수 만들어지는 과정\n",
    "* 오즈 비 > 로짓 함수 > 시그모이드 함수"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 오즈 비\n",
    "$$OR(odds ratio)=\\frac{p}{1-p}(p = 성공 확률)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 로짓 함수\n",
    "$$logit(p) = \\log(\\frac{p}{1-p})$$\n",
    "$$z = \\log(\\frac{p}{1-p})$$\n",
    "p=0.5 일때 0이되고 p가 0과 1로 갈 수록 무한대로 음수와 양수가 됨"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시그모이드 함수\n",
    "로짓 함수에서 z에 대해 정리\n",
    "$$p = \\frac{1}{1+e^{-z}}$$\n",
    "S자 형태의 그래프"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 중간 정리\n",
    "선형 회귀에서 손실 함수로 제곱 오차를 사용했듯 로지스틱 회귀를 위한 로지스틱 손실 함수가 있다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-3 로지스틱 손실 함수를 경사 하강법에 적용"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 로지스틱 손실 함수\n",
    "다중 분류를 위한 손실 함수 크로스 엔트로피(cross entropy) 손실 함수의 이진 분류 버전\n",
    "$$L=-(y\\log(a) + (1-y)\\log(1-a))\\\\\n",
    "y: 타깃값, a: 활성화 함수 출력값$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 로지스틱 손실 함수 미분\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_i}L = -(y-a)x_i \\\\\n",
    "\\frac{\\partial}{\\partial b}L = -(y-a)1 \\\\\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 유도과정\n",
    "연쇄법칙(chain rule)사용\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_i} = \\frac{\\partial L}{\\partial a} \\frac{\\partial a}{\\partial z} \\frac{\\partial z}{\\partial w_i} \\\\\n",
    "\\frac{\\partial L}{\\partial a} = -(y\\frac{1}{a} -(1-y)\\frac{1}{1-a}) \\\\\n",
    "\\frac{\\partial a}{\\partial z} = a(1-a) \\\\\n",
    "\\frac{\\partial z}{\\partial w_i} = x_i\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 역전파로 가중치와 절편 업데이트\n",
    "* 가중치 업데이트\n",
    "$$w_i = w_i +(y-a)x_i$$\n",
    "* 절편 업데이트\n",
    "$$b = b +(y-a)1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.15 (default, Nov 24 2022, 14:38:14) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a13f5bb60a88f91a902d04e4c145b424e72f3cecdd63854619232637e50b1f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
